Models ablation study (modification are built on previous ones):

Global Unchanged Params: batch_size = 128 (to avoid collapse), 100 epochs

1. Base (code du prof)
D_loss: 0.0000 | G_loss: 59.2713
Collapse and unrealistic image

2. New betas and seperate lr for G/D (betas=(0.5, 0.999))
[Epoch 020] D_loss: 1.2865 | G_loss: 0.8965
Evaluating at epoch 20...
FID: 48.3177 | Precision: 0.1510 | Recall: 0.2739
[Epoch 090] D_loss: 1.0724 | G_loss: 1.2164
Evaluating at epoch 90...
FID: 37.7641 | Precision: 0.2015 | Recall: 0.3972
Reference: UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS (Radford et. al. 2016)

3. Soft labels (0.9 for real, 0.1 for fake)
[Epoch 100] D_loss: 1.1585 | G_loss: 1.1657
FID Score: 18.0329
Precision with k = 3: 0.5931
Recall with k = 3   : 0.3991
Reference: Improved Techniques for Training GANs (Salimans et. al. 2016)

4. Add small gaussian noise to real and fake images to prevent D from overfitting
[Epoch 100] D_loss: 1.1760 | G_loss: 1.1156
FID Score: 18.6556
Precision with k = 3: 0.5412
Recall with k = 3   : 0.4276
Reference: Amortised MAP Inference for Image Super-resolution (Sønderby et. al. 2016)





Miscellaneous:
1. Batch size
To avoid collapse we find that we must used batch_size around 
Reference :
- Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2018). “Progressive Growing of GANs for Improved Quality, Stability, and Variation.” ICLR 2018 => 16–64
- Miyato, T., & Koyama, M. (2018). “Spectral Normalization for Generative Adversarial Networks.” ICLR 2018.                                       => 64-128
- Lucic, M., Kurach, K., Michalski, M., Gelly, S., & Bousquet, O. (2018). “Are GANs Created Equal? A Large-Scale Study.” NeurIPS 2018.            => 32–128