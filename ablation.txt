Models ablation study (modification are built on previous ones):

Global Unchanged Params: batch_size = 128 (to avoid collapse), 100 epochs

1. Base (code du prof)
D_loss: 0.0000 | G_loss: 59.2713
Collapse and unrealistic image

2. Different lr D and G
[Epoch 100] D_loss: 1.0323 | G_loss: 1.3191
FID Score: 20.6749
Precision with k = 3: 0.5412
Recall with k = 3   : 0.3032

3. Soft labels (0.9 for real, 0.1 for fake)
[Epoch 100] D_loss: 1.1585 | G_loss: 1.1657
FID Score: 18.0329
Precision with k = 3: 0.5931
Recall with k = 3   : 0.3991
Reference: Improved Techniques for Training GANs (Salimans et. al. 2016)

4. Add small gaussian noise to real and fake images to prevent D from overfitting
[Epoch 100] D_loss: 1.1760 | G_loss: 1.1156
FID Score: 18.6556
Precision with k = 3: 0.5412
Recall with k = 3   : 0.4276
Reference: Amortised MAP Inference for Image Super-resolution (Sønderby et. al. 2016)

5. New betas for G/D (betas=(0.5, 0.999))
[Epoch 100] D_loss: 1.1950 | G_loss: 1.0634
FID Score: 10.8748
Precision with k = 3: 0.6543
Recall with k = 3   : 0.5704
Reference: UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS (Radford et. al. 2016)



Miscellaneous:
1. Batch size
To avoid collapse we find that we must used batch_size around 
Reference :
- Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2018). “Progressive Growing of GANs for Improved Quality, Stability, and Variation.” ICLR 2018 => 16–64
- Miyato, T., & Koyama, M. (2018). “Spectral Normalization for Generative Adversarial Networks.” ICLR 2018.                                       => 64-128
- Lucic, M., Kurach, K., Michalski, M., Gelly, S., & Bousquet, O. (2018). “Are GANs Created Equal? A Large-Scale Study.” NeurIPS 2018.            => 32–128